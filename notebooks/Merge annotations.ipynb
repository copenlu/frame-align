{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UUID splits to raw month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_path = Path(\"/projects/frame_align/data/text_uuid_splits/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_set_dict = {}\n",
    "for file in uuid_path.iterdir():\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        month = list(data.keys())[0]\n",
    "        if month not in month_set_dict:\n",
    "            month_set_dict[month] = []\n",
    "        month_set_dict[month].extend([file.stem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(month_set_dict, open(\"/projects/frame_align/data/text_month_set_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_set_dict = pickle.load(open(\"/projects/frame_align/data/text_month_set_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01_2023-08-31: 8\n",
      "2023-09-01_2023-09-30: 3\n",
      "2023-05-01_2023-05-31: 8\n",
      "2024-04-01_2024-04-30: 4\n",
      "2023-07-01_2023-07-31: 8\n",
      "2023-10-01_2023-10-31: 5\n",
      "2024-03-01_2024-03-31: 4\n",
      "2023-06-01_2023-06-30: 3\n",
      "2024-01-01_2024-01-31: 4\n",
      "2023-12-01_2023-12-31: 4\n",
      "2024-02-01_2024-02-29: 2\n",
      "2023-11-01_2023-11-30: 3\n"
     ]
    }
   ],
   "source": [
    "# Combine the text annotations for each month\n",
    "for month, sets in month_set_dict.items():\n",
    "    print(f\"{month}: {len(sets)}\")\n",
    "    month_annotations_text = []\n",
    "    for set_no in sets:\n",
    "        set_df = pd.read_json(f\"/projects/frame_align/data/annotated/text/textframes_{set_no}.jsonl\", lines=True)\n",
    "        month_annotations_text.append(set_df)\n",
    "    month_annotations_text = pd.concat(month_annotations_text)\n",
    "    month_annotations_text.to_json(f\"/projects/frame_align/data/annotated/text_combined/textframes_{month}.jsonl\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01_2023-08-31\n",
      "Text annotations:  75534 Vision annotations:  75442\n",
      "Common uuids:  75442\n",
      "Merged df length:  75534\n",
      "2023-09-01_2023-09-30\n",
      "Text annotations:  25811 Vision annotations:  25811\n",
      "Common uuids:  25811\n",
      "Merged df length:  25811\n",
      "2023-05-01_2023-05-31\n",
      "Text annotations:  75296 Vision annotations:  75296\n",
      "Common uuids:  75296\n",
      "Merged df length:  75296\n",
      "2024-04-01_2024-04-30\n",
      "Text annotations:  36269 Vision annotations:  36258\n",
      "Common uuids:  36258\n",
      "Merged df length:  36269\n",
      "2023-07-01_2023-07-31\n",
      "Text annotations:  71805 Vision annotations:  71723\n",
      "Common uuids:  71723\n",
      "Merged df length:  71805\n",
      "2023-10-01_2023-10-31\n",
      "Text annotations:  41016 Vision annotations:  41016\n",
      "Common uuids:  41016\n",
      "Merged df length:  41016\n",
      "2024-03-01_2024-03-31\n",
      "Text annotations:  36498 Vision annotations:  18161\n",
      "Common uuids:  16498\n",
      "Merged df length:  38161\n",
      "2023-06-01_2023-06-30\n",
      "Text annotations:  25567 Vision annotations:  25567\n",
      "Common uuids:  25567\n",
      "Merged df length:  25567\n",
      "2024-01-01_2024-01-31\n",
      "Text annotations:  39317 Vision annotations:  39307\n",
      "Common uuids:  39307\n",
      "Merged df length:  39317\n",
      "2023-12-01_2023-12-31\n",
      "Text annotations:  34561 Vision annotations:  34561\n",
      "Common uuids:  34561\n",
      "Merged df length:  34561\n",
      "2024-02-01_2024-02-29\n",
      "Text annotations:  14274 Vision annotations:  14274\n",
      "Common uuids:  14274\n",
      "Merged df length:  14274\n",
      "2023-11-01_2023-11-30\n",
      "Text annotations:  28096 Vision annotations:  28781\n",
      "Common uuids:  28096\n",
      "Merged df length:  28781\n"
     ]
    }
   ],
   "source": [
    "# Merge the text annotations with the vision annotations\n",
    "for month in month_set_dict.keys():\n",
    "    print(f\"{month}\")\n",
    "    text_path = Path(f\"/projects/frame_align/data/annotated/text_combined/textframes_{month}.jsonl\")\n",
    "    vision_path = Path(f\"/projects/frame_align/data/annotated/vision/visionframes_{month}_pixtral_anno.jsonl\")\n",
    "    assert text_path.exists()\n",
    "    assert vision_path.exists()\n",
    "    month_annotations_text = pd.read_json(text_path, lines=True)\n",
    "    try:\n",
    "        month_annotations_vision = pd.read_json(vision_path, lines=True)\n",
    "    except:\n",
    "        month_annos = []\n",
    "        with open(vision_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    json_data = json.loads(line)\n",
    "                    month_annos.append(json_data)\n",
    "                except:\n",
    "                    pass\n",
    "        month_annotations_vision = pd.DataFrame(month_annos)\n",
    "    print(\"Text annotations: \", len(month_annotations_text), \"Vision annotations: \", len(month_annotations_vision))\n",
    "    month_annotations_text.rename(columns={\"id\": \"uuid\", \"reason\":\"frames-list-justification\"}, inplace=True)\n",
    "    common_uuids = set(month_annotations_text[\"uuid\"]).intersection(set(month_annotations_vision[\"uuid\"]))\n",
    "    print(\"Common uuids: \", len(common_uuids))\n",
    "    month_annotations = month_annotations_text.merge(month_annotations_vision, on=\"uuid\", how=\"outer\")\n",
    "    print(\"Merged df length: \", len(month_annotations))\n",
    "    month_annotations.to_json(f\"/projects/frame_align/data/annotated/merged/raw/merged_anno_{month}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations:  506392\n"
     ]
    }
   ],
   "source": [
    "# Total number of annotations\n",
    "total_annotations = 0\n",
    "for month in month_set_dict.keys():\n",
    "    month_annotations = pd.read_json(f\"/projects/frame_align/data/annotated/merged/raw/merged_anno_{month}.jsonl\", lines=True)\n",
    "    total_annotations += len(month_annotations)\n",
    "print(\"Total succesful annotations: \", total_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed text annotations:  12288\n",
      "Failed vision annotations:  68411\n"
     ]
    }
   ],
   "source": [
    "# Failed annotations\n",
    "failed_text = 0\n",
    "failed_vision = 0\n",
    "text_path = Path(\"/projects/frame_align/data/annotated/text/\")\n",
    "vision_path = Path(\"/projects/frame_align/data/annotated/vision/\")\n",
    "for file in text_path.iterdir():\n",
    "    if file.suffix == '.tsv':\n",
    "        failed_df = pd.read_csv(file, sep=\"\\t\")\n",
    "        failed_text += len(failed_df)\n",
    "for file in vision_path.iterdir():\n",
    "    if file.suffix == '.tsv':\n",
    "        failed_df = pd.read_csv(file, sep=\"\\t\")\n",
    "        failed_vision += len(failed_df)\n",
    "print(\"Failed text annotations: \", failed_text)\n",
    "print(\"Failed vision annotations: \", failed_vision)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_short_dict = {'economic': 'economic',\n",
    " 'capacity and resources': 'cap&res',\n",
    " 'morality': 'morality',\n",
    " 'fairness and equality': 'fairness',\n",
    " 'legality, constitutionality and jurisprudence': 'legality',\n",
    " 'policy prescription and evaluation': 'policy',\n",
    " 'crime and punishment': 'crime',\n",
    " 'security and defense': 'security',\n",
    " 'health and safety': 'health',\n",
    " 'quality of life': 'quality_life',\n",
    " 'cultural identity': 'culture',\n",
    " 'public opinion': 'public_op',\n",
    " 'political': 'political',\n",
    " 'external regulation and reputation': 'regulation',\n",
    " 'other': 'other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_col_names = ['topic', 'topic_justification', 'entity-name', 'entity-gender',\n",
    "       'sentiment', 'sentiment-reason', 'frames-list',\n",
    "       'frames-list-justification', 'issue_frame', 'issue_frame_justification',\n",
    "       'uuid', 'title', 'vision_frames_frames-list', 'vision_frames_reason',\n",
    "       'entity_entity-name', 'entity_entity-gender', 'entity_sentiment',\n",
    "       'entity_sentiment-reason', 'image_url']\n",
    "clean_col_names = ['text-topic', 'text-topic-exp', 'text-entity-name', 'text-entity-gender','text-entity-sentiment', 'text-entity-sentiment-exp', 'text-generic-frame',\n",
    "       'text-generic-frame-exp', 'text-issue-frame', 'text-issue-frame-exp',\n",
    "       'uuid', 'title', 'img-generic-frame', 'img-frame-exp',\n",
    "       'img-entity-name', 'img-entity-gender', 'img-entity-sentiment',\n",
    "       'img-entity-sentiment-exp', 'image-url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = Path('/projects/frame_align/data/annotated/merged/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text nan count:  3702\n",
      "Vision nan count:  20225\n"
     ]
    }
   ],
   "source": [
    "nan_count_text = 0\n",
    "nan_count_vision = 0\n",
    "for month_file in raw_path.iterdir():\n",
    "    month_df = pd.read_json(month_file, orient='records', lines=True)\n",
    "    month_df = month_df[orig_col_names]\n",
    "    month_df.columns = clean_col_names\n",
    "    nan_count_text += month_df['text-generic-frame'].isna().sum()\n",
    "    nan_count_vision += month_df['img-generic-frame'].isna().sum()\n",
    "print(\"Text nan count: \", nan_count_text)\n",
    "print(\"Vision nan count: \", nan_count_vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Topics and Political Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = json.load(open(\"/projects/frame_align/data/annotated/topics/latest_topic_labels.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_leaning = {\"left\" : ['alternet.org', 'editor.cnn.com', 'democracynow.org', 'dailybeast.com', 'huffpost.com', 'theintercept.com','jacobin.com', 'motherjones.com', 'newyorker.com', 'slate.com',   'msnbc.com', 'vox.com'],\n",
    "'left_lean' : ['abcnews.com','apnews.com', 'theatlantic.com', 'bloomberg.com', 'cbsnews.com', 'insider.com', 'nbcnews.com', 'thenytimes.com', 'npr.com', 'politico.com', 'propublica.org', 'time.com', 'washingtonpost.com', 'yahoonews.com','usatoday.com', 'theguardian.com'],\n",
    "\"center\" : ['axios.com', 'bbc.com', 'forbes.com', 'newsweek.com', 'reuters.com', 'realclearpolitics.com', 'thehill.com'],\n",
    "\"right_lean\" : ['thedispatch.com', 'theepochtimes.com', 'foxbusiness.com', 'ijr.com', 'nypost.com', 'thepostmillennial.com', 'washingtonexaminer.com', 'washingtontimes.com'],\n",
    "\"right\" : ['theamericanconservative.com', 'theamericanspectator.com', 'breitbart.com', 'dailycaller.com', 'dailywire.com', 'dailymail.com', 'foxnews.com', 'newsmax.com', 'oann.com', 'thefederalist.com']}\n",
    "all_hosts = [values for k, v in political_leaning.items() for values in v]\n",
    "host_mapping = {host: k for k, v in political_leaning.items() for host in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_columns_to_keep = ['id', 'authors', 'date_publish', 'description','language', 'maintext', 'source_domain','url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-01_2024-02-29, OG length:  14274, Invalid removal: 9455, Post sports: 8482, Post non-english: 8482\n",
      " Post bbc filter: 8482\n",
      "2023-07-01_2023-07-31, OG length:  71805, Invalid removal: 48896, Post sports: 44141, Post non-english: 43066\n",
      " Post bbc filter: 40190\n",
      "2024-01-01_2024-01-31, OG length:  39317, Invalid removal: 26734, Post sports: 25029, Post non-english: 25029\n",
      " Post bbc filter: 25029\n",
      "2023-05-01_2023-05-31, OG length:  75296, Invalid removal: 51831, Post sports: 47695, Post non-english: 47105\n",
      " Post bbc filter: 43999\n",
      "2024-04-01_2024-04-30, OG length:  36269, Invalid removal: 24412, Post sports: 22696, Post non-english: 22696\n",
      " Post bbc filter: 22696\n",
      "2023-11-01_2023-11-30, OG length:  28781, Invalid removal: 19528, Post sports: 18497, Post non-english: 18497\n",
      " Post bbc filter: 18497\n",
      "2023-10-01_2023-10-31, OG length:  41016, Invalid removal: 28285, Post sports: 26095, Post non-english: 25853\n",
      " Post bbc filter: 25456\n",
      "2024-03-01_2024-03-31, OG length:  38161, Invalid removal: 10724, Post sports: 9952, Post non-english: 9952\n",
      " Post bbc filter: 9952\n",
      "2023-12-01_2023-12-31, OG length:  34561, Invalid removal: 23597, Post sports: 22386, Post non-english: 22386\n",
      " Post bbc filter: 22386\n",
      "2023-06-01_2023-06-30, OG length:  25567, Invalid removal: 17390, Post sports: 15961, Post non-english: 15849\n",
      " Post bbc filter: 15046\n",
      "2023-09-01_2023-09-30, OG length:  25811, Invalid removal: 17181, Post sports: 15539, Post non-english: 15454\n",
      " Post bbc filter: 14547\n",
      "2023-08-01_2023-08-31, OG length:  75534, Invalid removal: 51635, Post sports: 46424, Post non-english: 45216\n",
      " Post bbc filter: 42248\n"
     ]
    }
   ],
   "source": [
    "for month_file in raw_path.iterdir():\n",
    "    month_df = pd.read_json(month_file, orient='records', lines=True)\n",
    "    month_name = month_file.stem[12:]\n",
    "    print(f\"{month_name}, OG length: \", len(month_df), end=\",\")\n",
    "    # Clean column names\n",
    "    month_df = month_df[orig_col_names]\n",
    "    month_df.columns = clean_col_names\n",
    "    # Drop NaN framing values\n",
    "    month_df.dropna(subset=['text-generic-frame', 'img-generic-frame'], inplace=True)\n",
    "    # Shorten frame names\n",
    "    clean_frame_preds_text = month_df['text-generic-frame'].apply(lambda frame_list: [frame_short_dict[frame.lower()] for frame in frame_list if frame.lower() in frame_short_dict])\n",
    "    clean_frame_preds_text_len = clean_frame_preds_text.apply(len)\n",
    "    clean_frame_preds_img = month_df['img-generic-frame'].apply(lambda frame_list: [frame_short_dict[frame.lower()] for frame in frame_list if frame.lower() in frame_short_dict])\n",
    "    clean_frame_preds_img_len = clean_frame_preds_img.apply(len)\n",
    "    month_df[(clean_frame_preds_text_len < 0) | (clean_frame_preds_img_len < 0)].to_csv(f\"/projects/frame_align/data/annotated/merged/bad_preds/{month_name}_badpreds.csv\", index=False)\n",
    "    # month_df['text-generic-frame'] = \n",
    "    month_df['img-generic-frame-len'] = clean_frame_preds_img_len\n",
    "    month_df['text-generic-frame-len'] = clean_frame_preds_text_len\n",
    "    month_df = month_df[(month_df['img-generic-frame-len'] > 0) & (month_df['text-generic-frame-len'] > 0)]\n",
    "    print(f\" Invalid removal: {len(month_df)}\", end=\",\")\n",
    "    # Add GPT topics\n",
    "    month_df['gpt-topic'] = month_df['text-topic'].apply(lambda x: topic_mapping[x] if x in topic_mapping else None)\n",
    "    month_df = month_df[month_df['gpt-topic'] != \"Sports\"]\n",
    "    print(f\" Post sports: {len(month_df)}\", end=\",\")\n",
    "    orig_df = pd.read_csv(f\"/projects/frame_align/data/raw/text/{month_name}/datawithtopics_merged.csv\")\n",
    "    merged_df = month_df.merge(orig_df[orig_columns_to_keep], left_on='uuid', right_on='id', how='left')\n",
    "    merged_df.drop(columns=['id'], inplace=True)\n",
    "    # Only keeping english articles\n",
    "    merged_df = merged_df[merged_df['language'] == 'en']\n",
    "    print(f\" Post non-english: {len(merged_df)}\", end=\",\")\n",
    "    # Remove bbc and dailymail\n",
    "    merged_df = merged_df[~merged_df['source_domain'].isin(['www.bbc.com', 'www.dailymail.com'])]\n",
    "    print(f\" Post bbc filter: {len(merged_df)}\")\n",
    "    # Add political leaning\n",
    "    political_leaning = []\n",
    "    for row_no, row in merged_df.iterrows():\n",
    "        for host in all_hosts:\n",
    "            if host in row['source_domain']:\n",
    "                political_leaning.append(host_mapping[host])\n",
    "                break\n",
    "        else:\n",
    "            political_leaning.append(None)\n",
    "    assert len(political_leaning) == len(merged_df)\n",
    "    merged_df['political_leaning'] = political_leaning\n",
    "    month_df.to_json(f\"/projects/frame_align/data/annotated/merged/processed/json/{month_file.stem}.jsonl\", orient=\"records\", lines=True)\n",
    "    month_df.to_csv(f\"/projects/frame_align/data/annotated/merged/processed/csv/{month_file.stem}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed annotations:  302897\n"
     ]
    }
   ],
   "source": [
    "# Total number of annotations\n",
    "total_annotations = 0\n",
    "combined_annotations = []\n",
    "for month in month_set_dict.keys():\n",
    "    month_annotations = pd.read_json(f\"/projects/frame_align/data/annotated/merged/processed/json/merged_anno_{month}.jsonl\", lines=True)\n",
    "    combined_annotations.append(month_annotations)\n",
    "    total_annotations += len(month_annotations)\n",
    "print(\"Total processed annotations: \", total_annotations)\n",
    "combined_annotations = pd.concat(combined_annotations)\n",
    "combined_annotations.to_json(\"/projects/frame_align/data/annotated/merged/processed/combined_annotations.jsonl\", orient=\"records\", lines=True)\n",
    "combined_annotations.to_csv(\"/projects/frame_align/data/annotated/merged/processed/combined_annotations.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frame-align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
