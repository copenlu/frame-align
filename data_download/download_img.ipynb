{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/vsl333/datasets/news-bert-data/bertopic/allcsvtopics\"\n",
    "image_dir = \"/projects/belongielab/data/frame-align\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def download_images(directory_name, id_list, image_url_list):\n",
    "    for i in tqdm(range(len(id_list))):\n",
    "        id = id_list[i]\n",
    "        image_url = image_url_list[i]\n",
    "\n",
    "        # download image using url and save to image_dir/directory_path. Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.join(image_dir, directory_name), exist_ok=True)\n",
    "        # image_path = os.path.join(image_dir, directory_name, f\"{id}.jpg\")\n",
    "        # download image url to image_path. Add timeout to prevent hanging\n",
    "        try:\n",
    "            response = requests.get(image_url, stream=True, timeout=20)  # Add a timeout (in seconds)\n",
    "            response.raise_for_status()  # Raise an HTTPError if the status is not 200\n",
    "            raw_image = Image.open(response.raw).convert(\"RGB\")\n",
    "            \n",
    "            # Check the shape of the image tensor\n",
    "            image_tensor = torch.tensor(np.array(raw_image))\n",
    "            if image_tensor.shape[-1] != 3:\n",
    "                raise ValueError(f\"Unexpected image shape: {image_tensor.shape}\")\n",
    "            if image_tensor.shape[0] == 1 and image_tensor.shape[1] == 1:\n",
    "                logger.info(f\"Skipping image with shape {image_tensor.shape} - id: {id}\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Image URL: {image_url}\")\n",
    "            logger.error(f\"Image error {e} - id: {id}\")\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(image_dir, directory_name, f\"{id}.jpg\")\n",
    "        raw_image.save(image_path)\n",
    "        logger.info(f\"Saved image to {image_path}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def filter_urls(base_dir, directory_name):\n",
    "    directory_path = os.path.join(base_dir, directory_name)\n",
    "    csv_file = \"datawithtopics_merged.csv\"\n",
    "    df = pd.read_csv(os.path.join(directory_path, csv_file))\n",
    "\n",
    "    # get 'id' and 'image_url' columns and drop rows with missing image_url\n",
    "    df_image = df[['id', 'image_url']].dropna(subset=['image_url']) \n",
    "\n",
    "    df_image = df_image[0:10]\n",
    "\n",
    "    id_list = df_image['id'].tolist()\n",
    "    image_url_list = df_image['image_url'].tolist()\n",
    "\n",
    "    return df, id_list, image_url_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Download images from URLs.\")\n",
    "    # for base_dir too\n",
    "    parser.add_argument(\"base_dir\", type=str, help=\"The base directory to save images.\")\n",
    "    parser.add_argument(\"directory_name\", type=str, help=\"The directory name to save images.\")\n",
    "    base_dir = \"/home/vsl333/datasets/news-bert-data/bertopic/allcsvtopics\"\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_df, id_list, image_url_list = filter_urls(args.directory_name)\n",
    "    download_images(args.directory_name, id_list, image_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-11-01_2023-11-30', '2023-06-01_2023-06-30', '2024-01-01_2024-01-31', '2024-03-01_2024-03-31', '2023-08-01_2023-08-31', '2024-02-01_2024-02-29', '2024-04-01_2024-04-30', '2023-05-01_2023-05-31', '2023-10-01_2023-10-31', '2023-12-01_2023-12-31', '2023-07-01_2023-07-31', '2023-09-01_2023-09-30']\n"
     ]
    }
   ],
   "source": [
    "# give list of director names in the base_dir, not path, just names\n",
    "dir_list = []\n",
    "for directory_name in os.listdir(base_dir):\n",
    "    if os.path.isdir(os.path.join(base_dir, directory_name)):\n",
    "        dir_list.append(directory_name)\n",
    "\n",
    "print(dir_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frame-align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
